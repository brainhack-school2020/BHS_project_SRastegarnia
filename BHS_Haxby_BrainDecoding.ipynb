{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haxby data set:\n",
    "Haxby is a high-quality block-design fMRI dataset from a study on face & object representation in the human ventral temporal cortex (This cortex is involved in the high-level visual processing of complex stimuli). It consists of 6 subjects with 12 runs per subject. In this experiment during each run, the subjects passively viewed greyscale images of 8 object categories, grouped in 24s blocks separated by rest periods. Each image was shown for 500ms and was followed by a 1500ms inter-stimulus interval.\n",
    "\n",
    "## Project Goal\n",
    "For this project i am trying machine learning and deep learning methods to learning about barin decoding and predicting which object category the subject saw by analyzing the fMRI activity recorded masks of the ventral stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from nilearn.plotting import plot_anat, show, plot_stat_map, plot_matrix\n",
    "from nilearn import datasets, plotting, image\n",
    "from nilearn.image import mean_img, get_data \n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier,  RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we don't define which subject by default 2nd subject will be fetched.\n",
    "haxby_ds = datasets.fetch_haxby(subjects=[4], fetch_stimuli=True)\n",
    "# print(haxby_ds)\n",
    "\n",
    "# To define which subject to be fetched:\n",
    "# haxby_ds = datasets.fetch_haxby(subjects=[], fetch_stimuli=True)\n",
    "\n",
    "len(haxby_ds.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['anat', 'func', 'session_target', 'mask_vt', 'mask_face', 'mask_house', 'mask_face_little', 'mask_house_little', 'mask', 'description', 'stimuli'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look inside the data\n",
    "haxby_ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_file = haxby_ds.mask\n",
    "labels = haxby_ds.session_target[0]\n",
    "mask_vt_file = haxby_ds.mask_vt[0]\n",
    "mask_face_file = haxby_ds.mask_face[0]\n",
    "\n",
    "# 'func' is a list of filenames: one for each subject\n",
    "func_file = haxby_ds.func[0]\n",
    "\n",
    "# Load the behavioral data that I will predict\n",
    "beh_label = pd.read_csv(haxby_ds.session_target[0], sep=\" \")\n",
    "\n",
    "# Extract tags indicating to which acquisition run a tag belongs\n",
    "session = beh_label['chunks']\n",
    "\n",
    "# Preparing the data (Load target information as string and give a numerical identifier to each)\n",
    "stimuli_lable = beh_label['labels']\n",
    "\n",
    "# Identify the resting state\n",
    "nonrest_task_mask = (y != 'rest')\n",
    "\n",
    "# Remove the resting state and find names of remaining active labels\n",
    "categories = stimuli_lable[nonrest_task_mask].unique()\n",
    "#session = session[nonrest_task_mask]\n",
    "\n",
    "# Get the labels of the numerical conditions represented by the vector y\n",
    "unique_conditions, order = np.unique(categories, return_index=True)\n",
    "\n",
    "# Sort the conditions by the order of appearance\n",
    "unique_conditions = unique_conditions[np.argsort(order)]\n",
    "\n",
    "# Extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = beh_label['chunks'][nonrest_task_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional nifti images are located at: /home/srastegarnia/nilearn_data/haxby2001/subj4/bold.nii.gz\n",
      "Mask nifti image (3D) is located at:  /home/srastegarnia/nilearn_data/haxby2001/mask.nii.gz\n",
      "First subject functional nifti images (4D) are at: /home/srastegarnia/nilearn_data/haxby2001/subj4/bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Print basic information on the dataset\n",
    "print('Functional nifti images are located at: %s' % haxby_ds.func[0])\n",
    "print('Mask nifti image (3D) is located at:  %s' % haxby_ds.mask)\n",
    "print('First subject functional nifti images (4D) are at: %s' %func_file)  # 4D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rest 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rest 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rest 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rest 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rest 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels chunks\n",
       "0        rest 0\n",
       "1        rest 0\n",
       "2        rest 0\n",
       "3        rest 0\n",
       "4        rest 0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkout the confounds of the data\n",
    "session_target = pd.read_csv(haxby_ds['session_target'][0], sep='\\t')\n",
    "\n",
    "session_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the fMRI data (smooth and apply the mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decoding, standardizing is important, I am also smoothing the data\n",
    "\n",
    "nifti_masker = NiftiMasker(mask_img=mask_file, standardize=True, sessions=session,  smoothing_fwhm=4,\n",
    "                           memory=\"nilearn_cache\", memory_level=1)\n",
    "\n",
    "X = nifti_masker.fit_transform(func_file)\n",
    "\n",
    "\n",
    "# Remove the resting state\n",
    "#X = X[nonrest_task_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Haxby masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = NiftiMasker(mask_img=mask_vt_file, standardize=True)\n",
    "fmri_masked = masker.fit_transform(func_file)\n",
    "\n",
    "# Report depict the computed mask\n",
    "# masker.generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.6038213  -4.024481    0.82268804 ... -1.3077999  -0.76046395\n",
      "  -0.80006576]\n",
      " [-2.6749918  -4.101693    1.2567352  ... -1.4760997  -0.8379458\n",
      "  -0.6758426 ]\n",
      " [-3.1494625  -3.5612085   1.0739785  ... -1.1395003  -1.0962186\n",
      "  -0.80006576]\n",
      " ...\n",
      " [-1.4176446  -0.7429663  -1.5303041  ... -0.992238   -0.5538457\n",
      "  -0.9656967 ]\n",
      " [-1.038068   -1.6502086  -1.6902162  ... -0.5083763  -0.81211853\n",
      "  -0.75865805]\n",
      " [-0.7296622  -0.91669357 -1.484615   ...  0.5645344   0.24679996\n",
      "  -0.1789499 ]]\n"
     ]
    }
   ],
   "source": [
    "# The variable “fmri_masked” is a numpy array\n",
    "print(fmri_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1452, 675)\n"
     ]
    }
   ],
   "source": [
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Mask to a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 64, 64, 1452)\n",
      "(40, 64, 64)\n",
      "39912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srastegarnia/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# load bold image into memory as a nibabel image\n",
    "func = nib.load(func_file)\n",
    "\n",
    "# load mask image into memory as a nibabel image\n",
    "mask = nib.load(mask_file) \n",
    "\n",
    "# get the physical data of the mask (3D matrix of voxels)\n",
    "mask_data = mask.get_data() \n",
    "\n",
    "print(func.shape)\n",
    "print(mask.shape) \n",
    "print(len(mask_data[mask_data==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1452, 39912)\n"
     ]
    }
   ],
   "source": [
    "# Create the masker object \n",
    "masker = NiftiMasker(mask_img=mask_file, standardize=True)\n",
    "\n",
    "# Create a numpy matrix from the BOLD data, using the mask for the transformation\n",
    "#%memit bold_masked = masker.fit_transform(bold_path)\n",
    "func_masked = masker.fit_transform(func_file)\n",
    "\n",
    "# View the dimensions of the matrix. The shape represents the number of time-stamps by the number of voxels in the mask. \n",
    "print(func_masked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.851505   -1.1370772  -1.029912   ... -0.97751945 -0.9890595\n",
      "  -0.9778365 ]\n",
      " [ 1.6732124  -1.1370772  -1.029912   ... -0.97751945 -0.9890595\n",
      "  -0.9778365 ]\n",
      " [-1.0308924  -1.1370772  -1.029912   ... -0.97751945  1.6580281\n",
      "   1.1377147 ]\n",
      " ...\n",
      " [ 0.12800965  0.75440264  0.08345481 ... -0.97751945 -0.9890595\n",
      "  -0.9778365 ]\n",
      " [ 0.6331721   0.8390958   0.32418278 ... -0.97751945 -0.9890595\n",
      "  -0.9778365 ]\n",
      " [ 0.5143103   1.064944   -0.12718217 ... -0.97751945 -0.9890595\n",
      "  -0.9778365 ]]\n"
     ]
    }
   ],
   "source": [
    "# Viewing the numerical values of the matrix\n",
    "print(func_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels from a csv into an array using pandas\n",
    "stimuli = pd.read_csv(labels, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1452, 2)\n",
      "     labels  chunks\n",
      "0      rest       0\n",
      "1      rest       0\n",
      "2      rest       0\n",
      "3      rest       0\n",
      "4      rest       0\n",
      "...     ...     ...\n",
      "1447   rest      11\n",
      "1448   rest      11\n",
      "1449   rest      11\n",
      "1450   rest      11\n",
      "1451   rest      11\n",
      "\n",
      "[1452 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# View the dimensions of the matrix\n",
    "print(stimuli.shape)\n",
    "\n",
    "# Viewing the values of the matrix\n",
    "print(stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       rest\n",
      "1       rest\n",
      "2       rest\n",
      "3       rest\n",
      "4       rest\n",
      "        ... \n",
      "1447    rest\n",
      "1448    rest\n",
      "1449    rest\n",
      "1450    rest\n",
      "1451    rest\n",
      "Name: labels, Length: 1452, dtype: object\n"
     ]
    }
   ],
   "source": [
    "targets = stimuli['labels']\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       False\n",
      "1       False\n",
      "2       False\n",
      "3       False\n",
      "4       False\n",
      "        ...  \n",
      "1447    False\n",
      "1448    False\n",
      "1449    False\n",
      "1450    False\n",
      "1451    False\n",
      "Name: labels, Length: 1452, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "targets_mask = targets.isin(['face', 'cat'])\n",
    "print(targets_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 39912)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_masked = func_masked[targets_mask]\n",
    "func_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216,)\n",
      "6       face\n",
      "7       face\n",
      "8       face\n",
      "9       face\n",
      "10      face\n",
      "        ... \n",
      "1370     cat\n",
      "1371     cat\n",
      "1372     cat\n",
      "1373     cat\n",
      "1374     cat\n",
      "Name: labels, Length: 216, dtype: object\n"
     ]
    }
   ],
   "source": [
    "targets_masked = targets[targets_mask]\n",
    "\n",
    "print(targets_masked.shape)\n",
    "print(targets_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different classifiers for decoding\n",
    "In this section I am willing to compare the different classifiers on a visual object recognition decoding task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bottle       0.88      1.00      0.93         7\n",
      "         cat       0.90      0.69      0.78        13\n",
      "       chair       0.88      0.88      0.88         8\n",
      "        face       0.92      0.86      0.89        14\n",
      "       house       1.00      0.92      0.96        13\n",
      "        rest       0.84      0.92      0.88        63\n",
      "    scissors       0.89      0.89      0.89         9\n",
      "scrambledpix       0.91      0.91      0.91        11\n",
      "        shoe       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.88       146\n",
      "   macro avg       0.91      0.87      0.89       146\n",
      "weighted avg       0.89      0.88      0.88       146\n",
      "\n",
      "Test score with L1 penalty: 0.8836\n"
     ]
    }
   ],
   "source": [
    "#select data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = beh_label['labels']\n",
    "\n",
    "#shuffle the data and split the sample into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "#standarize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Support vector classifier\n",
    "svm = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='linear',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "#accuracy\n",
    "score = svm.score(X_test, y_test)\n",
    "\n",
    "#print classification report\n",
    "svm = svm.predict(X_test)\n",
    "report = classification_report(y_test, svm)\n",
    "print(report)\n",
    "\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity with L1 penalty: 0.00%\n",
      "Test score with L1 penalty: 0.8151\n"
     ]
    }
   ],
   "source": [
    "#select data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = beh_label['labels']\n",
    "\n",
    "#shuffle the data and split the sample into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "#standarize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#multinomial logistic regression object using L1 penalty\n",
    "logistic_50 = LogisticRegression(C=50., multi_class='multinomial', penalty='l1', solver='saga', tol=0.1)\n",
    "\n",
    "#train model\n",
    "logistic_50.fit(X_train, y_train)\n",
    "sparsity = np.mean(logistic_50.coef_ == 0) * 100\n",
    "score = logistic_50.score(X_test, y_test)\n",
    "\n",
    "# print('Best C % .4f' % clf.C_)\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bottle       0.31      0.80      0.44         5\n",
      "         cat       0.64      0.82      0.72        11\n",
      "       chair       0.80      0.92      0.86        13\n",
      "        face       0.88      1.00      0.93        14\n",
      "       house       0.80      0.62      0.70        13\n",
      "        rest       0.93      0.64      0.76        64\n",
      "    scissors       0.80      0.80      0.80        10\n",
      "scrambledpix       0.42      1.00      0.59         5\n",
      "        shoe       0.75      0.82      0.78        11\n",
      "\n",
      "    accuracy                           0.75       146\n",
      "   macro avg       0.70      0.82      0.73       146\n",
      "weighted avg       0.82      0.75      0.76       146\n",
      "\n",
      "Accuracy on test set: 75.342%\n"
     ]
    }
   ],
   "source": [
    "#select data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = beh_label['labels']\n",
    "\n",
    "#shuffle the data and split the sample into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "#standarize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#kneighbors classifier object\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='kd_tree',\n",
    "                           p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "#fit model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#response prediction\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "knn.score(X_test, y_test)\n",
    "\n",
    "#print classification report\n",
    "knn = knn.predict(X_test)\n",
    "report = classification_report(y_test, knn)\n",
    "print(report)\n",
    "\n",
    "#evaluate accuracy\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bottle       0.50      0.12      0.20         8\n",
      "         cat       0.15      0.22      0.18         9\n",
      "       chair       0.38      0.45      0.42        11\n",
      "        face       0.78      0.54      0.64        13\n",
      "       house       0.80      0.73      0.76        11\n",
      "        rest       0.77      0.75      0.76        67\n",
      "    scissors       0.56      0.42      0.48        12\n",
      "scrambledpix       0.13      0.29      0.18         7\n",
      "        shoe       0.20      0.25      0.22         8\n",
      "\n",
      "    accuracy                           0.56       146\n",
      "   macro avg       0.47      0.42      0.43       146\n",
      "weighted avg       0.61      0.56      0.58       146\n",
      "\n",
      "Test score with L1 penalty: 0.5616\n"
     ]
    }
   ],
   "source": [
    "#select data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = beh_label['labels']\n",
    "\n",
    "#shuffle the data and split the sample into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "#standarize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "#train model\n",
    "dtc.fit(X_train, y_train)\n",
    "score = dtc.score(X_test, y_test)\n",
    "\n",
    "#print classification report\n",
    "dtc = dtc.predict(X_test)\n",
    "report = classification_report(y_test, dtc)\n",
    "print(report)\n",
    "\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "Class MLPClassifier implements a multi-layer perceptron algorithm that trains using Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srastegarnia/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bottle       0.25      0.50      0.33         6\n",
      "         cat       0.38      0.38      0.38         8\n",
      "       chair       0.33      0.50      0.40        10\n",
      "        face       0.77      0.67      0.71        15\n",
      "       house       0.42      0.45      0.43        11\n",
      "        rest       0.89      0.77      0.82        73\n",
      "    scissors       0.33      0.44      0.38         9\n",
      "scrambledpix       0.67      0.33      0.44         6\n",
      "        shoe       0.50      0.50      0.50         8\n",
      "\n",
      "    accuracy                           0.63       146\n",
      "   macro avg       0.50      0.50      0.49       146\n",
      "weighted avg       0.68      0.63      0.65       146\n",
      "\n",
      "Test score with L1 penalty: 0.6301\n"
     ]
    }
   ],
   "source": [
    "#select data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = beh_label['labels']\n",
    "\n",
    "#shuffle the data and split the sample into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "#standarize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#multinomial logistic regression object using L1 penalty\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "MLPClassifier(activation='logistic', alpha=1e-05, batch_size='auto',\n",
    "              beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "              epsilon=1e-08, hidden_layer_sizes=(5, 2),\n",
    "              max_iter=200, momentum=0.9, n_iter_no_change=10,\n",
    "              nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
    "              solver='lbfgs', tol=0.0001, validation_fraction=0.1, \n",
    "              verbose=False, warm_start=False)\n",
    "\n",
    "#train model\n",
    "nn.fit(X_train, y_train)\n",
    "score = nn.score(X_test, y_test)\n",
    "\n",
    "#print classification report\n",
    "nn = nn.predict(X_test)\n",
    "report = classification_report(y_test, nn)\n",
    "print(report)\n",
    "\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction accuracy\n",
    "# cv_scores_svc = cross_val_score(MLPClassifier, X_train, y_train, cv=5) \n",
    "# print(cv_scores)\n",
    "\n",
    "# # The mean prediction accuracy\n",
    "# classification_accuracy = np.mean(cv_scores_svc)\n",
    "# classification_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [864, 864, 1452]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-55519da82a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [864, 864, 1452]"
     ]
    }
   ],
   "source": [
    "# Run time for all these classifiers\n",
    "\n",
    "# Make a data splitting object for cross validation\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "classifiers_scores = {}\n",
    "\n",
    "for classifier_name, classifier in sorted(classifiers.items()):\n",
    "    classifiers_scores[classifier_name] = {}\n",
    "    print(70 * '_')\n",
    "\n",
    "    for category in categories:\n",
    "        classification_target = y[nonrest_task_mask].isin([category])\n",
    "        t0 = time.time()\n",
    "        classifiers_scores[classifier_name][category] = cross_val_score(\n",
    "            classifier,\n",
    "            masked_timecourses,\n",
    "            classification_target,\n",
    "            cv=cv,\n",
    "            groups=session_labels,\n",
    "            scoring=\"f1\",\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"%10s: %14s -- scores: %1.2f +- %1.2f, time %.2fs\" %\n",
    "            (\n",
    "                classifier_name,\n",
    "                category,\n",
    "                classifiers_scores[classifier_name][category].mean(),\n",
    "                classifiers_scores[classifier_name][category].std(),\n",
    "                time.time() - t0,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary, to hold all our classifiers\n",
    "classifiers = {'SVC': svm,\n",
    "               'LogisticRegression': logistic_50,\n",
    "               'KNeighborsClassifier':knn,\n",
    "               'DecisionTreeClassifier': dtc,\n",
    "               'MLPClassifier': nn,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = beh_label['labels']\n",
    "\n",
    "#shuffle the data and split the sample into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "#standarize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# masked_timecourses = masker.fit_transform(func_file)[nonrest_task_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = beh_label[\"chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, array(['scrambledpix', 'rest', 'rest', 'rest', 'rest', 'rest', 'scissors',\n       'rest', 'scrambledpix', 'rest', 'rest', 'house', 'cat',\n       'scrambledpix', 'scissors', 'rest', 'face', 'rest', 'scrambledpix',\n       'cat', 'rest', 'rest', 'rest', 'cat', 'scissors', 'rest', 'rest',\n       'rest', 'scrambledpix', 'shoe', 'face', 'cat', 'rest', 'scissors',\n       'chair', 'rest', 'rest', 'rest', 'house', 'rest', 'scrambledpix',\n       'chair', 'cat', 'shoe', 'house', 'shoe', 'scrambledpix', 'face',\n       'rest', 'chair', 'rest', 'rest', 'rest', 'shoe', 'rest', 'chair',\n       'scrambledpix', 'rest', 'cat', 'rest', 'rest', 'scissors', 'rest',\n       'rest', 'rest', 'rest', 'rest', 'face', 'shoe', 'scrambledpix',\n       'rest', 'rest', 'scrambledpix', 'rest', 'scissors', 'rest', 'rest',\n       'face', 'rest', 'rest', 'rest', 'rest', 'scissors', 'cat', 'rest',\n       'face', 'shoe', 'rest', 'chair', 'house', 'chair', 'house',\n       'house', 'cat', 'house', 'house', 'scrambledpix', 'scrambledpix',\n       'face', 'house', 'rest', 'shoe', 'rest', 'cat', 'shoe', 'rest',\n       'rest', 'chair', 'scissors', 'cat', 'cat', 'rest', 'rest', 'rest',\n       'chair', 'scissors', 'rest', 'scrambledpix', 'chair', 'chair',\n       'scrambledpix', 'bottle', 'rest', 'rest', 'chair', 'face', 'rest',\n       'cat', 'rest', 'bottle', 'rest', 'rest', 'rest', 'shoe', 'rest',\n       'face', 'house', 'rest', 'shoe', 'scrambledpix', 'chair', 'rest',\n       'rest', 'cat', 'chair', 'rest'], dtype=object) was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6331bac1496a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \"\"\"\n\u001b[1;32m    382\u001b[0m     \u001b[0;31m# To ensure multimetric format is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         raise TypeError(\"estimator should be an estimator implementing \"\n\u001b[0;32m--> 401\u001b[0;31m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, array(['scrambledpix', 'rest', 'rest', 'rest', 'rest', 'rest', 'scissors',\n       'rest', 'scrambledpix', 'rest', 'rest', 'house', 'cat',\n       'scrambledpix', 'scissors', 'rest', 'face', 'rest', 'scrambledpix',\n       'cat', 'rest', 'rest', 'rest', 'cat', 'scissors', 'rest', 'rest',\n       'rest', 'scrambledpix', 'shoe', 'face', 'cat', 'rest', 'scissors',\n       'chair', 'rest', 'rest', 'rest', 'house', 'rest', 'scrambledpix',\n       'chair', 'cat', 'shoe', 'house', 'shoe', 'scrambledpix', 'face',\n       'rest', 'chair', 'rest', 'rest', 'rest', 'shoe', 'rest', 'chair',\n       'scrambledpix', 'rest', 'cat', 'rest', 'rest', 'scissors', 'rest',\n       'rest', 'rest', 'rest', 'rest', 'face', 'shoe', 'scrambledpix',\n       'rest', 'rest', 'scrambledpix', 'rest', 'scissors', 'rest', 'rest',\n       'face', 'rest', 'rest', 'rest', 'rest', 'scissors', 'cat', 'rest',\n       'face', 'shoe', 'rest', 'chair', 'house', 'chair', 'house',\n       'house', 'cat', 'house', 'house', 'scrambledpix', 'scrambledpix',\n       'face', 'house', 'rest', 'shoe', 'rest', 'cat', 'shoe', 'rest',\n       'rest', 'chair', 'scissors', 'cat', 'cat', 'rest', 'rest', 'rest',\n       'chair', 'scissors', 'rest', 'scrambledpix', 'chair', 'chair',\n       'scrambledpix', 'bottle', 'rest', 'rest', 'chair', 'face', 'rest',\n       'cat', 'rest', 'bottle', 'rest', 'rest', 'rest', 'shoe', 'rest',\n       'face', 'house', 'rest', 'shoe', 'scrambledpix', 'chair', 'rest',\n       'rest', 'cat', 'chair', 'rest'], dtype=object) was passed"
     ]
    }
   ],
   "source": [
    "# Run time for all these classifiers\n",
    "\n",
    "# Make a data splitting object for cross validation\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "classifiers_scores = {}\n",
    "\n",
    "for classifier_name, classifier in sorted(classifiers.items()):\n",
    "    classifiers_scores[classifier_name] = {}\n",
    "    print(70 * '_')\n",
    "\n",
    "    for category in categories:\n",
    "        classification_target = y.isin([category])\n",
    "        t0 = time.time()\n",
    "        classifiers_scores[classifier_name][category] = cross_val_score(\n",
    "            classifier,\n",
    "            X,\n",
    "            y,\n",
    "            cv=5,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"%10s: %14s -- scores: %1.2f +- %1.2f, time %.2fs\" %\n",
    "            (\n",
    "                classifier_name,\n",
    "                category,\n",
    "                classifiers_scores[classifier_name][category].mean(),\n",
    "                classifiers_scores[classifier_name][category].std(),\n",
    "                time.time() - t0,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a rudimentary diagram\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "tick_position = np.arange(len(categories))\n",
    "plt.xticks(tick_position, categories, rotation=45)\n",
    "\n",
    "for color, classifier_name in zip(\n",
    "        [#'#E8D0A9', '#B7AFA3', '#771E10', \n",
    "         '#48110C', '#808080', '#DB4C2C', '#E38C2D', '#EBC137'],\n",
    "        sorted(classifiers)):\n",
    "    score_means = [classifiers_scores[classifier_name][category].mean()\n",
    "                   for category in categories]\n",
    "    plt.bar(tick_position, score_means, label=classifier_name, width=.11, color=color)\n",
    "    tick_position = tick_position + .09\n",
    "\n",
    "plt.ylabel('Classification accurancy (f1 score)')\n",
    "plt.xlabel('Visual stimuli category')\n",
    "plt.ylim(ymin=0)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title('Category-specific classification accuracy for different classifiers')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the face vs house map for the different classifiers\n",
    "\n",
    "mean_epi_img = image.mean_img(func_file)\n",
    "\n",
    "# Restrict the decoding to face vs house\n",
    "condition_mask = y.isin(['face', 'house'])\n",
    "masked_timecourses = masked_timecourses[\n",
    "    condition_mask[nonrest_task_mask]]\n",
    "y_f = (y[condition_mask] == 'face')\n",
    "# Transform the stimuli to binary values\n",
    "y_f.astype(np.int)\n",
    "\n",
    "for classifier_name, classifier in sorted(classifiers.items()):\n",
    "    classifier.fit(masked_timecourses, y_f)\n",
    "\n",
    "    if hasattr(classifier, 'coef_'):\n",
    "        weights = classifier.coef_[0]\n",
    "    elif hasattr(classifier, 'best_estimator_'):\n",
    "        weights = classifier.best_estimator_.coef_[0]\n",
    "    else:\n",
    "        continue\n",
    "    weight_img = masker.inverse_transform(weights)\n",
    "    weight_map = get_data(weight_img)\n",
    "    threshold = np.max(np.abs(weight_map)) * 1e-3\n",
    "    plot_stat_map(weight_img, bg_img=mean_epi_img, display_mode='z', cut_coords=[-15],\n",
    "                  threshold=threshold, title='%s: face vs house' % classifier_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffling\n",
    "# X_train, X_test, y_train, y_test=train_test_split(func_masked, targets_masked, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with ANOVA + SVM: face vs house in the Haxby dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the analysis to faces and places\n",
    "condition_mask = beh_label['labels'].isin(['face', 'house'])\n",
    "conditions_f_h = y[condition_mask]\n",
    "\n",
    "# Confirm that I now have 2 conditions\n",
    "print(conditions_f_h.unique())\n",
    "\n",
    "# Record these as an array of sessions, with fields\n",
    "# for condition (face or house) and run\n",
    "session_f_h = beh_label[condition_mask].to_records(index=False)\n",
    "print(session_f_h.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our condition_mask to fMRI data\n",
    "X_f_h = X[condition_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the decoder\n",
    "\n",
    "#Define the prediction function to be used. Here I am using a Support Vector Classification, with a linear kernel\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Define the dimension reduction to be used. (keep 5% of voxels)\n",
    "feature_selection = SelectPercentile(f_classif, percentile=5)\n",
    "\n",
    "# I have SVC classifier and our feature selection (SelectPercentile),then plug them together in a *pipeline*:\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the decoder and predict\n",
    "anova_svc.fit(X_f_h, conditions_f_h)\n",
    "y_pred = anova_svc.predict(X_f_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain prediction scores via cross validation\n",
    "\n",
    "# Define the cross-validation scheme used for validation, using LeaveOneGroupOut cross-validation.\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "# Compute the prediction accuracy for the different folds (i.e. session)\n",
    "cv_scores = cross_val_score(anova_svc, X_f_h, conditions_f_h, cv=cv, groups=session_f_h)\n",
    "\n",
    "# Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" % (classification_accuracy, 1. / len(conditions_f_h.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the SVC’s discriminating weights\n",
    "coef = svc.coef_\n",
    "# reverse feature selection\n",
    "coef = feature_selection.inverse_transform(coef)\n",
    "# reverse masking\n",
    "weight_img = masker.inverse_transform(coef)\n",
    "\n",
    "# Use the mean image as a background to avoid relying on anatomical data\n",
    "mean_img = image.mean_img(func_file)\n",
    "\n",
    "# Create the figure\n",
    "plot_stat_map(weight_img, mean_img, title='SVM weights')\n",
    "\n",
    "# Save the results as a Nifti file\n",
    "#weight_img.to_file('haxby_face_vs_house.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI-based decoding analysis\n",
    "In this section, I am looking at decoding accuracy for different objects in three different masks: the full ventral stream (mask_vt), the house selective areas (mask_house), and the face-selective areas (mask_face), that have been defined via a standard General Linear Model (GLM) based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = beh_label[\"chunks\"][nonrest_task_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifier: a support vector classifier\n",
    "classifier = SVC(C=1., kernel=\"linear\")\n",
    "\n",
    "# A classifier to set the chance level\n",
    "dummy_classifier = DummyClassifier()\n",
    "\n",
    "# Make a data splitting object for cross validation\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "mask_names = ['mask_vt', 'mask_face', 'mask_house']\n",
    "\n",
    "mask_scores = {}\n",
    "mask_chance_scores = {}\n",
    "\n",
    "for mask_name in mask_names:\n",
    "    print(\"Working on mask %s\" % mask_name)\n",
    "    \n",
    "    # Standardizing\n",
    "    mask_filename = haxby_ds[mask_name][0]\n",
    "    masker = NiftiMasker(mask_img=mask_filename, standardize=True)\n",
    "    masked_timecourses = masker.fit_transform(func_file)[nonrest_task_mask]\n",
    "\n",
    "    mask_scores[mask_name] = {}\n",
    "    mask_chance_scores[mask_name] = {}\n",
    "\n",
    "    for category in categories:\n",
    "        print(\"Processing %s %s\" % (mask_name, category))\n",
    "        classification_target = (y[nonrest_task_mask] == category)\n",
    "        mask_scores[mask_name][category] = cross_val_score(\n",
    "            classifier,\n",
    "            masked_timecourses,\n",
    "            classification_target,\n",
    "            cv=cv,\n",
    "            groups=session_labels,\n",
    "            scoring=\"roc_auc\",\n",
    "        )\n",
    "\n",
    "        mask_chance_scores[mask_name][category] = cross_val_score(\n",
    "            dummy_classifier,\n",
    "            masked_timecourses,\n",
    "            classification_target,\n",
    "            cv=cv,\n",
    "            groups=session_labels,\n",
    "            scoring=\"roc_auc\",\n",
    "        )\n",
    "\n",
    "        print(\"Scores: %1.2f +- %1.2f\" % (\n",
    "            mask_scores[mask_name][category].mean(),\n",
    "            mask_scores[mask_name][category].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different multi-class strategies\n",
    "I compare one vs all and one vs one multi-class strategies: the overall cross-validated accuracy and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoders, using scikit-learn\n",
    "\n",
    "svc_ovo = OneVsOneClassifier(Pipeline([\n",
    "    ('anova', SelectKBest(f_classif, k=500)),\n",
    "    ('svc', SVC(kernel='linear'))\n",
    "]))\n",
    "\n",
    "svc_ova = OneVsRestClassifier(Pipeline([\n",
    "    ('anova', SelectKBest(f_classif, k=500)),\n",
    "    ('svc', SVC(kernel='linear'))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"rest\" condition\n",
    "y = y[nonrest_task_mask]\n",
    "X = X[nonrest_task_mask]\n",
    "\n",
    "cv_scores_ovo = cross_val_score(svc_ovo, X, y, cv=5, verbose=1)\n",
    "\n",
    "cv_scores_ova = cross_val_score(svc_ova, X, y, cv=5, verbose=1)\n",
    "\n",
    "print('OvO:', cv_scores_ovo.mean())\n",
    "print('OvA:', cv_scores_ova.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barplots of the prediction scores\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.boxplot([cv_scores_ova, cv_scores_ovo])\n",
    "plt.xticks([1, 2], ['One vs All', 'One vs One'])\n",
    "plt.title('Prediction: accuracy score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I fit on the the first 5 sessions and plot a confusion matrix on the last 2 sessions\n",
    "\n",
    "svc_ovo.fit(X[session_labels < 5], y[session_labels < 5])\n",
    "y_pred_ovo = svc_ovo.predict(X[session_labels >= 5])\n",
    "\n",
    "plot_matrix(confusion_matrix(y_pred_ovo, y[session_labels >= 5]),\n",
    "            labels=unique_conditions, cmap='hot_r')\n",
    "plt.title('Confusion matrix: One vs One')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "svc_ova.fit(X[session_labels < 5], y[session_labels < 5])\n",
    "y_pred_ova = svc_ova.predict(X[session_labels >= 5])\n",
    "\n",
    "plot_matrix(confusion_matrix(y_pred_ova, y[session_labels >= 5]),\n",
    "            labels=unique_conditions, cmap='hot_r')\n",
    "plt.title('Confusion matrix: One vs All')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
