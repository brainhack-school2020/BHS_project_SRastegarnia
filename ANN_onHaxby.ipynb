{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an artificial neural network (ANN)?\n",
    "ANN is the foundation of AI and one of the main tools used in ML inspired by the human brain when networks of neurons \n",
    "analyze and process information. ANNs have self-learning capabilities and can learn from their experience that enables them \n",
    "to produce better results as more data becomes available.\n",
    "In this computational model, neural networks consist of input and output layers also there is a hidden layer (in most cases)\n",
    "that transforms the input to the usable data for the output layer. ANN initially goes through a training phase. During this \n",
    "supervised phase, the network is taught what to look for and what is the desired output. The difference between the results \n",
    "is adjusted using backpropagation (going from output to input) in order to achieve the lowest possible difference between \n",
    "the actual and desired outcome.\n",
    "\n",
    "In this notebook, I trained ANN model on the HAXBY dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import datasets, plotting, image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nilearn.plotting import plot_anat, show, plot_stat_map, plot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate input file\n",
    "haxby_ds = datasets.fetch_haxby(subjects=[4], fetch_stimuli=True)\n",
    "\n",
    "# 'func' is a list of filenames: one for each subject\n",
    "func_file = haxby_ds.func[0]\n",
    "\n",
    "# Standardizing\n",
    "mask_vt_file = haxby_ds.mask_vt[0]\n",
    "masker = NiftiMasker(mask_img=mask_vt_file, standardize=True)\n",
    "\n",
    "# Load the behavioral data that I will predict\n",
    "beh_label = pd.read_csv(haxby_ds.session_target[0], sep=\" \")\n",
    "\n",
    "# Regressout the resting state\n",
    "nonrest_task_mask = (beh_label['labels'] != 'rest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting data\n",
    "\n",
    "# X contains the features\n",
    "X = masker.fit_transform(func_file)[nonrest_task_mask]\n",
    "\n",
    "# y contains the target variable\n",
    "y = beh_label['labels'][nonrest_task_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the resting state and find names of remaining active labels\n",
    "categories = y[nonrest_task_mask].unique()\n",
    "\n",
    "# Get the labels of the numerical conditions represented by the vector y\n",
    "unique_conditions, order = np.unique(categories, return_index=True)\n",
    "\n",
    "# Sort the conditions by the order of appearance\n",
    "unique_conditions = unique_conditions[np.argsort(order)]\n",
    "\n",
    "# Extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = beh_label['chunks'][nonrest_task_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the string to numerical values. (ML Algorithm can only work on numbers and not on string)\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(864,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 6\n",
      " 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 0 0 0\n",
      " 0 0 0 0 0 0 7 7 7 7 7 7 7 7 7 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 3 3 3 3\n",
      " 3 3 3 3 3 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3\n",
      " 3 3 3 1 1 1 1 1 1 1 1 1 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5\n",
      " 5 5 2 2 2 2 2 2 2 2 2 7 7 7 7 7 7 7 7 7 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 6 6 6 6 6 6 6 6 6 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 7 7 7 7 7 7 7 7 7\n",
      " 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 1\n",
      " 1 1 1 1 1 1 1 1 7 7 7 7 7 7 7 7 7 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 0 0\n",
      " 0 0 0 0 0 0 0 4 4 4 4 4 4 4 4 4 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 6 6 6\n",
      " 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 7 7 7 7 7 7 7 7 7 2 2 2 2 2 2 2 2 2 1 1 1 1\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 4 4 4 4 4\n",
      " 4 4 4 4 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6\n",
      " 6 6 6 0 0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3\n",
      " 3 3 1 1 1 1 1 1 1 1 1 7 7 7 7 7 7 7 7 7 4 4 4 4 4 4 4 4 4 6 6 6 6 6 6 6 6\n",
      " 6 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5\n",
      " 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 7 7 7 7 7 7 7 7 7 1\n",
      " 1 1 1 1 1 1 1 1 6 6 6 6 6 6 6 6 6 0 0 0 0 0 0 0 0 0 4 4 4 4 4 4 4 4 4 2 2\n",
      " 2 2 2 2 2 2 2 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 7 7 7 7 7 7 7 7 7 1 1 1\n",
      " 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 1 1 1 1\n",
      " 1 1 1 1 1 4 4 4 4 4 4 4 4 4 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 0 0 0 0 0\n",
      " 0 0 0 0 5 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [7],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.reshape(y, (864,1))\n",
    "y = test\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(864, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>864 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7\n",
       "0    0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "1    0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "2    0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "3    0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "4    0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...\n",
       "859  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "860  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "861  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "862  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "863  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "\n",
       "[864 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "y = pd.DataFrame(enc.fit_transform(y).toarray())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split the data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state = 0)\n",
    "\n",
    "#standarize features caling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(777, 675)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srastegarnia/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(338, input_dim=675, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(338 , input_dim = 675, init = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srastegarnia/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(169, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(169, init = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(8, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 338)               228488    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 169)               57291     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 1360      \n",
      "=================================================================\n",
      "Total params: 287,139\n",
      "Trainable params: 287,139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srastegarnia/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "777/777 [==============================] - 1s 1ms/step - loss: 1.7123 - accuracy: 0.3861\n",
      "Epoch 2/100\n",
      "777/777 [==============================] - 0s 460us/step - loss: 0.9158 - accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "777/777 [==============================] - 0s 501us/step - loss: 0.4550 - accuracy: 0.8507\n",
      "Epoch 4/100\n",
      "777/777 [==============================] - 0s 461us/step - loss: 0.2605 - accuracy: 0.9254\n",
      "Epoch 5/100\n",
      "777/777 [==============================] - 1s 678us/step - loss: 0.1398 - accuracy: 0.9524\n",
      "Epoch 6/100\n",
      "777/777 [==============================] - 1s 807us/step - loss: 0.0964 - accuracy: 0.9755\n",
      "Epoch 7/100\n",
      "777/777 [==============================] - 0s 616us/step - loss: 0.0474 - accuracy: 0.9884\n",
      "Epoch 8/100\n",
      "777/777 [==============================] - 0s 622us/step - loss: 0.0300 - accuracy: 0.9923\n",
      "Epoch 9/100\n",
      "777/777 [==============================] - 0s 478us/step - loss: 0.0396 - accuracy: 0.9884\n",
      "Epoch 10/100\n",
      "777/777 [==============================] - 1s 657us/step - loss: 0.0828 - accuracy: 0.9807\n",
      "Epoch 11/100\n",
      "777/777 [==============================] - 1s 800us/step - loss: 0.0814 - accuracy: 0.9820\n",
      "Epoch 12/100\n",
      "777/777 [==============================] - 0s 470us/step - loss: 0.4159 - accuracy: 0.8970\n",
      "Epoch 13/100\n",
      "777/777 [==============================] - 0s 452us/step - loss: 0.4181 - accuracy: 0.8662\n",
      "Epoch 14/100\n",
      "777/777 [==============================] - 0s 478us/step - loss: 0.0603 - accuracy: 0.9833\n",
      "Epoch 15/100\n",
      "777/777 [==============================] - 0s 480us/step - loss: 0.0247 - accuracy: 0.9910\n",
      "Epoch 16/100\n",
      "777/777 [==============================] - 0s 478us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "777/777 [==============================] - 0s 481us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "777/777 [==============================] - 0s 536us/step - loss: 8.3995e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "777/777 [==============================] - 0s 472us/step - loss: 7.0902e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "777/777 [==============================] - 0s 470us/step - loss: 6.1831e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "777/777 [==============================] - 0s 559us/step - loss: 5.4590e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "777/777 [==============================] - 0s 502us/step - loss: 4.8858e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "777/777 [==============================] - 0s 472us/step - loss: 4.3916e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "777/777 [==============================] - 0s 579us/step - loss: 4.0041e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "777/777 [==============================] - 0s 476us/step - loss: 3.6452e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "777/777 [==============================] - 0s 480us/step - loss: 3.3471e-04 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "777/777 [==============================] - 1s 702us/step - loss: 3.0844e-04 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "777/777 [==============================] - 0s 498us/step - loss: 2.8493e-04 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "777/777 [==============================] - 0s 531us/step - loss: 2.6437e-04 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "777/777 [==============================] - 1s 708us/step - loss: 2.4583e-04 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "777/777 [==============================] - 0s 457us/step - loss: 2.2864e-04 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "777/777 [==============================] - 0s 473us/step - loss: 2.1344e-04 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "777/777 [==============================] - 0s 481us/step - loss: 1.9971e-04 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "777/777 [==============================] - 0s 476us/step - loss: 1.8739e-04 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "777/777 [==============================] - 0s 465us/step - loss: 1.7554e-04 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "777/777 [==============================] - 0s 477us/step - loss: 1.6535e-04 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "777/777 [==============================] - 0s 581us/step - loss: 1.5556e-04 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "777/777 [==============================] - 0s 472us/step - loss: 1.4626e-04 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "777/777 [==============================] - 1s 687us/step - loss: 1.3801e-04 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "777/777 [==============================] - 0s 467us/step - loss: 1.3039e-04 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "777/777 [==============================] - 0s 471us/step - loss: 1.2326e-04 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "777/777 [==============================] - 0s 456us/step - loss: 1.1650e-04 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "777/777 [==============================] - 0s 452us/step - loss: 1.1053e-04 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "777/777 [==============================] - 0s 464us/step - loss: 1.0441e-04 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "777/777 [==============================] - 0s 449us/step - loss: 9.8945e-05 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "777/777 [==============================] - 0s 484us/step - loss: 9.3895e-05 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "777/777 [==============================] - 0s 461us/step - loss: 8.9002e-05 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "777/777 [==============================] - 0s 448us/step - loss: 8.4705e-05 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "777/777 [==============================] - 0s 509us/step - loss: 8.0421e-05 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "777/777 [==============================] - 0s 497us/step - loss: 7.6450e-05 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "777/777 [==============================] - 1s 677us/step - loss: 7.2884e-05 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "777/777 [==============================] - 0s 582us/step - loss: 6.9314e-05 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "777/777 [==============================] - 0s 576us/step - loss: 6.5818e-05 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "777/777 [==============================] - 0s 580us/step - loss: 6.2761e-05 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "777/777 [==============================] - 1s 697us/step - loss: 5.9763e-05 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "777/777 [==============================] - 1s 651us/step - loss: 5.7073e-05 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "777/777 [==============================] - 0s 500us/step - loss: 5.4382e-05 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "777/777 [==============================] - 0s 541us/step - loss: 5.1871e-05 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "777/777 [==============================] - 0s 535us/step - loss: 4.9487e-05 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "777/777 [==============================] - 0s 479us/step - loss: 4.7301e-05 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "777/777 [==============================] - 0s 515us/step - loss: 4.5216e-05 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "777/777 [==============================] - 0s 494us/step - loss: 4.3098e-05 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "777/777 [==============================] - 0s 488us/step - loss: 4.1097e-05 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "777/777 [==============================] - 0s 473us/step - loss: 3.9274e-05 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "777/777 [==============================] - 0s 479us/step - loss: 3.7668e-05 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "777/777 [==============================] - 0s 496us/step - loss: 3.5988e-05 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "777/777 [==============================] - 0s 539us/step - loss: 3.4392e-05 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "777/777 [==============================] - 0s 638us/step - loss: 3.2861e-05 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "777/777 [==============================] - 0s 491us/step - loss: 3.1416e-05 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "777/777 [==============================] - 0s 614us/step - loss: 3.0045e-05 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "777/777 [==============================] - 0s 484us/step - loss: 2.8808e-05 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "777/777 [==============================] - 0s 505us/step - loss: 2.7511e-05 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "777/777 [==============================] - 0s 561us/step - loss: 2.6309e-05 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "777/777 [==============================] - 0s 459us/step - loss: 2.5214e-05 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "777/777 [==============================] - 0s 490us/step - loss: 2.4138e-05 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "777/777 [==============================] - 0s 496us/step - loss: 2.3130e-05 - accuracy: 1.0000\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777/777 [==============================] - 0s 537us/step - loss: 2.2106e-05 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "777/777 [==============================] - 0s 428us/step - loss: 2.1171e-05 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "777/777 [==============================] - 0s 419us/step - loss: 2.0295e-05 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "777/777 [==============================] - 0s 473us/step - loss: 1.9454e-05 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "777/777 [==============================] - 0s 436us/step - loss: 1.8623e-05 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "777/777 [==============================] - 0s 440us/step - loss: 1.7832e-05 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "777/777 [==============================] - 0s 432us/step - loss: 1.7040e-05 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "777/777 [==============================] - 0s 427us/step - loss: 1.6341e-05 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "777/777 [==============================] - 0s 521us/step - loss: 1.5664e-05 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "777/777 [==============================] - 0s 428us/step - loss: 1.5009e-05 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "777/777 [==============================] - 0s 422us/step - loss: 1.4381e-05 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "777/777 [==============================] - 0s 424us/step - loss: 1.3787e-05 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "777/777 [==============================] - 0s 421us/step - loss: 1.3205e-05 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "777/777 [==============================] - 0s 424us/step - loss: 1.2672e-05 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "777/777 [==============================] - 0s 422us/step - loss: 1.2143e-05 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "777/777 [==============================] - 0s 418us/step - loss: 1.1628e-05 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "777/777 [==============================] - 0s 428us/step - loss: 1.1183e-05 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "777/777 [==============================] - 0s 451us/step - loss: 1.0718e-05 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "777/777 [==============================] - 0s 420us/step - loss: 1.0261e-05 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "777/777 [==============================] - 0s 428us/step - loss: 9.8421e-06 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "777/777 [==============================] - 0s 426us/step - loss: 9.4288e-06 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "777/777 [==============================] - 0s 432us/step - loss: 9.0271e-06 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "777/777 [==============================] - 0s 435us/step - loss: 8.6591e-06 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "777/777 [==============================] - 0s 443us/step - loss: 8.2939e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f918823efd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions and evaluating the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  0  0  0  0  1  0  0]\n",
      " [ 0 13  0  0  0  0  0  0]\n",
      " [ 1  0 11  0  2  1  0  0]\n",
      " [ 1  0  0 14  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0]\n",
      " [ 0  0  1  0  0  8  0  1]\n",
      " [ 0  0  0  0  0  0  6  0]\n",
      " [ 2  0  0  0  0  0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7]), <a list of 8 Text yticklabel objects>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAFaCAYAAADfDmAgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de9xlY/3/8dd7ZhjHMaZBaDQIhaTc6UCR0whf6ZdE8TWljJSUsxRTCZFD3+Rw5zAkp86lg5QkibolSaSD8/EezBjDjPD5/XFdN3u2PXPve+3T2rPfz8djP+6991p7rc9e997rsz/Xda21FBGYmZkVMarTAZiZWfdyEjEzs8KcRMzMrDAnETMzK8xJxMzMCnMSMTOzwpxEWkjSbZK2zPcl6XxJT0j6o6R3SPpHHcv4kKRf1rm+6ZIuajDslpM0WVJIGtPpWBqR38NrOh1HLZKOlTRT0sMNLGMNSU9JGt3M2Dolv5e1Oh3H4sZJZBEkXZN3+mPrmHeGpGMrn4uIDSLimvxwc2Bb4FURsWlE/C4i1htuuRHx7YjYrkj8vUjS3ZK26XQcnSRpEnAwsH5EvLLociLi3ohYLiKeb150zZe/px8dbr78Xv7Tjph6iZPIQkiaDLwDCGDnYeat55faq4G7I2Juw8GV3OLyy3VhuqCCejXwWEQ82ulAyqAL/l/dLSJ8q3EDjgZ+D5wCXFE1bQZwJvAzYC6wL/Bf4FngKeAneb67gW2AfYB5wPN5+heALYH7K5Y5Cfg+MAg8Bpyen58KXFcx39eA+4AngZuAd1RMmw5ctJD3syVwP+kX6qPAQ8CHK6aPBb4K3As8ApwFLF0rhvxcAK9ZyPbYBtgRuDnHeR8wveK1k/Prxywk1ruBQ4C/ArOBy4ClKqbvBPwFmAVcD2yUn/8W8ALwTN7OhwEXAAfn6avn9e6fH78GeBxQfvwx4F/5uR8Dq1W9308A/wTuqrENNs/v812AgFPzdp6d38eGC3mvE4DzgQeBJ4AfVkwbLp79cjxPAN/I690mv/8X8jaYQdVnrfKzme9vCgzk/9UjwCm1/k/AajmOx3NcH6v67F0OXAjMAW4D+hbx/Qpg/xz/HOBLwNrAH3IclwNL5nlXBK4gfTeeyPdflad9mfS9mpff7+nD/b+AJUmfnwPy86NJ3/WjO73f6cZbxwMo6y1/SfYHNiEliFUqps3IO4fNSNXcUvm5Y6uWUflFncqCyeDFL3b+EN9C2vEsm5e3+UJetyfwCmAMKSE8TN7BMnwSeQ74IrAEsAPwNLBinn5a3kFMAJYHfgIcXyuG/Fx1EqneHlsCr8+PNyLtnHbJ809m+CTyR9JOawJwO7BfnvYm0s75LXm77Z3nH1u9zfPjj/BSUv8g8G/gsoppP8r3twJm5uWPBb4OXFv1fq/K8SxduQ2AKaQEsml+fgopwY8n7dhfB6y6kPf6U1KSXDH/X7YYQTxX5HWsQdrBbl/92ar1uMZn8w/AXvn+csBba/2fgN8CZ+T/78Z5nVtXfPbmkT5Xo4HjgRsW8f0K0udtHLABMB/4NbAWsALwd2DvPO8rgPcBy5A+m99hwWR7DfDRGsuv+f/K9zckJaTXAUcBNwCjO73f6cZbxwMo4430q/K/wMT8+A7gMxXTZwAXVr1mBsWTyNvyF/JlO9Xq19WY/gTwhnx/OotOIs9UroO0M34raUc3F1i7YtrbeOkX3Mti4OVJ5MKFxZjnOQ04Nd9fYOdUY967gT0rHp8InJXvnwl8qWr+f/DSzvfFbZ4fr02qWEaRqqtpFdv9AuCgfP9c4MSK1y2XPwOTK97vVjW2wZHAPcDrK57fCrgzb9tRi9gmq5IqhhVrTKsnns0rpl8OHFH92ar1uMZn81pSdTyxap4X/0+kSvl5YPmK6ccDMyo+e7+qmLY+8Mwi3nsAm1U8vgk4vOLxycBpC3ntxsATFY+voXYSqfX/ek3F44NJ3+0ngHUW9fn1beE394nUtjfwy4iYmR9fnJ+rdF8T1zcJuCcinhtuRkkHS7pd0mxJs0i/2ibWuZ7HqtbxNGnntBLpV95Nkmbl5f4iP1+vBbaHpLdI+o2kQUmzSU0v9cYJqcKqjhNSe//BQ3HmWCeRqpaXiYh/k5o5Nib1cV0BPChpPWAL0q9r8uvvqXjdU6RmxdUX9h6zTwOXR8StFa+9Gjid1MT0iKR+SeNqvHYS8HhEPFFjWj3xLGwbjdQ+wLrAHZL+JGmnhcTzeETMqXjunmHiWWqY/ohHKu4/U+PxcgCSlpF0tqR7JD1JSnrj6+h7G+47egEpUf4sIv45zLy2EE4iVSQtDewGbCHp4TxE8jPAGyS9oWLWqHpp9eORuA9YY7gOQEnvAA7P8a0YEeNJzUhqYN2Qmk2eATaIiPH5tkJEDO2U5pKSzFActUb8VL//i0nNFZMiYgVSFdBonJC21Zcr4hwfEctExCULiQNSotiV1Mb+QH78v6QmpL/keR4kJSgAJC1LakZ5oGI5tZb9fmAXSZ+ufDIi/i8iNiE11awLHLqQ9zJB0vga0+qJp17V/7/RVPxAiIh/RsQewMrAV4Dv5vVVxzNB0vIVz61RMJ6ROhhYD3hLRIwD3pmfH/o8Ley7N9x38gzSj4opkjZvOMoe5STycruQyvb1Sb9eNya1m/6OtONZmEdI7blF/JHU0X2CpGUlLSVpsxrzLU/q1xgExkg6mtSm3JCIeAH4JnCqpJUBJK0uaUqe5RZgA0kbS1qK1HQxnOVJv1znSdqU1B/RDN8E9suVjvL22rFi51br//Bb4JOkX7CQmj8OIDXRDQ1fvRj4cH6PY4HjgBsj4u5h4nkQ2Br4lKT9ASS9Oce3BGkHPjSoYgER8RDwc+AMSStKWkLS0A6yaDy13EmqCnbMMX2O1M9CjndPSSvlz8Gs/PQC8UbEfaRBDMfnz+dGpArm2wXiGanlST9yZkmaABxTNX3E3z1Je5H6O6cCnwIukFS0kutpTiIvtzdwfqQx8g8P3UjNEx9aRLVwLrB+bmL54UhWmHdk/0PqpL2XNIrqAzVmvZK007mT1JQwj+Y1qx1OGkxwQ24y+BXp1x8RcSepQ/5XpNEu19WxvP2BL0qaQxrpdnkzgoyIAdKopdNJbdn/Iu0IhhwPfC7/Hw7Jz/2WtCMaSiLXkX6ZDz0mIn4NfB74Himhrw3sXmdM95ISyeH5eIVxpGT3BOn/9Bhp5Fste5H6Ou4g9VF9utF4asQ3m/T/OIdUOcwlfcaGbA/cJukp0ui/3SNiXo1F7UFq/nkQ+AFwTERcVSSmEToNWJpUMd9Aamqt9DVg13xM1/8NtzBJa+Rl/m9EPBURF5NGp53a3LB7w9DQRjMzsxFzJWJmZoU5iZiZWWFOImZmVpiTiJmZFeYkYmZmhfXk2S0njlVMLsOI8DU36XQEVmpPdjqArOFDkRYrN91008yIGMnZHBZrPZlEJi8HA1OGn6/lLh7odARWald2OoCsDF+W8pB0z/Bz9Q43Z5mZWWFOImZmVpiTiJmZFeYkYmZmhTmJmJlZYU4iZmZWmJOImZkV5iRiZmaFOYmYmVlhTiJmZlaYk4iZmRXmJGJmZoU5iZiZWWFOImZmVpiTiJmZFVbqJCJpbUk/lLRLE5a1r6QBSQOD85oRnZmZlf2iVB8nxfh6SVsA/42IwyS9G3g/MBv4ErAbsB6wAnBYRMysXlBE9AP9AH2vULQpfjOzxVrZk8gVwL+AJfNtK0krAR8BdouIkLQcsBdwFTAa2Ai4ukPxmpn1lLInEYBXAGtExDRJk4BlgYiIoWpCwH0RMb1TAZqZ9apS94lks4AVJB0CrJOfO19Sv6QTgSWAP0v6uqTTJb22Y5GamfUYvfSDvnf0vUIxMKXTUQAX9962t5G4stMBZGX4spSHpJsioq/TcZRFN1QiZmbWRJLOk/SopL/VmHaIpJA0sZ5lOYmYmfWeGcD21U/mfudtgXvrXZCTiJlZj4mIa4HHa0w6FTgMqLut3UnEzMyQtDPwQETcMpLXdcMQXzOznjdGqnsY1AtwG1B5bo7+fMB1TZKWAY4CthtxXCN9gZmZtV+QDpKrxxyYN8IRZGsDawK3SAJ4FenQiU0j4uFFvdBJxMysC4h0So5WiIhbgZVfXJd0N9BX6xRS1dwnYmbWJUbVeRuOpEuAPwDrSbpf0j5FY3IlYmbWBZpZiUTEHsNMn1zvspxEzMy6RKuasxrhJGJm1gVEOfsfejOJrLkJXDzQ6Sh4exoF0XHX9+D507qDz1llLxHpehhl05tJxMysC7kSMTOzQlo5xLcRTiJmZl3ClYiZmRXiSsTMzAoT6TKuZeMkYmbWBVyJmJlZQ5xEzMysEB9saGZmDXElYmZmhbgSMTOzwnzaEzMza4grETMzK8RDfM3MrCFOImZmVog71s3MrLCynvakjIltAZJmSFpqmHlOqGM5+0oakDQwODjYvADNzNpgqE+knls7lTKJSFpT0rcknQKsAhwm6fuSNpW0mqQvSzpT0s75JZMlTZZ0raQjJE2oXmZE9EdEX0T0rbTSSm19P2ZmzeAkUr/9gaMj4iDgEeBs4EhgR+A5UlX3KLBX1etui4gTIuLxdgZrZtZqQ30i9dzaqaxJBOCFivuzgfnAWFLi+AFwHLB81Wtmtyc0M7P2a1YlIuk8SY9K+lvFcydJukPSXyX9QNL4emIqaxI5A/iCpK8AK1dNux6YBhwEPNvuwMzMOqHJfSIzgO2rnrsK2DAiNgLuJLX+DKuUo7Mi4i5gatXTdwNH5Pt/qJp/93z3CMzMFkPNHJ0VEddKmlz13C8rHt4A7FrPskqZRMzM7OXa2HT0EeCyemZ0EjEz6wIjPO3JREkDFY/7I6K/rvVIR5EGMH27nvmdRMzMusQIksjMiOgb6fIl7Q3sBGwdEVHPa5xEzMy6QKtPeyJpe+BwYIuIeLre15V1dJaZmVVp4hDfS0gDlNaTdL+kfYDTSYdNXCXpL5LOqicmVyJmZl2gyaOz9qjx9LlFluUkYmbWBXw9ETMza0gZ+x+cRMzMuoArETMzK8xJxMzMCivrRal6NIk8CVzZ6SC4vr5jeVrubKnTIQAwrSTbI53bswymdDqArCxxmCsRMzMrxNdYNzOzhrgSMTOzQlyJmJlZQ1yJmJlZIR6dZWZmhfk4ETMza4iTiJmZFeKOdTMzK8zNWWZm1hBXImZmVoiAJTsdRA1OImZmXcB9ImZm1hD3iZiZWSHuWDczs4a4OcvMzAop62lPSpPYJJ3QwGuHvaqSpH0lDUgaGBycXXRVZmYdMdScVc+tndpaiUjaA9gGmAPcDrwRmB8RBwKTJb0W+BJwH3Ae6ZJqawGzIuIoSV8ExgErANOAfuDfwK2SdgSeAm6NiPOq1x0R/Xl++vrWLcsl9MzM6tasBCHpPGAn4NGI2DA/NwG4DJgM3A3sFhFPDLesdlcik4BbgJOBbSNiv5xAhowHHgcujIi/AasBfwROkzQOWD0iPg1cA2yXX9MfET8EJgC/Bi5uyzsxM2ujoSG+9dzqMAPYvuq5I4BfR8Q6pH3pEfUsqK1JJCJOBK4FTq217oi4ATgW2FXS3sChwJ3At0jbcKiCqKwkhtqm9sp/v9X8yM3MOq9ZzVkRcS3pB3ul9wAX5PsXALvUE1O7m7OmAesCzwF/knQ6MC8iDsnT3wXsDLwC+DlwODARGATmAg9JOplUdUwDds2vWwI4BXgauKOd78nMrB1GOMR3oqSBisf9uUl/UVaJiIcAIuIhSSvXs6K2JpGIOHsR03bPd39T8fTvq2b7fNXjqRX39ysemZlZuY1wdNbMiOhrWTAVSjM6y8zMFq2JfSK1PCJpVYD899F6YzIzs5JrwxDfHwN75/t7Az+q50U+2NDMrAs087Qnki4BtiT1ndwPHAOcAFwuaR/gXuD99SzLScTMrEs0q+koIvZYyKStR7osJxEzsy7gEzCamVlhZT13lpOImVmXcCViZmaF+MqGZmbWEFciZmZWiDvWzcysMHesW2lNi3JcXmXZ4a8t1hZzS7I9zCq5EjEzs4a4Y93MzApxJWJmZg1xJWJmZoW4EjEzs4aMrrcUeaGlYSzAScTMrBuMpBRxEjEzs5epN4n8t6VRLMBJxMysG5T05FlOImZm3aKEPetOImZm3aCk5z1xEjEz6wYlHePrJGJm1i3cJ2JmZoW4EjEzs4aUMImUsDgyM7OXGRriW8+tnsVJn5F0m6S/SbpE0lJFwnISMTPrBgKWrPM23KKk1YFPAX0RsSGpxtm9SFhdkUQkXdqEZewraUDSwODg7GaEZWbWXk2sREjdGUtLGgMsAzxYJKTS9olI2gPYBpgDrCDpaGBT4JOkXHsM8AzwE+CnwJeBscBzEXFI9fIioh/oB+jrW9eXrjOz7jKyjvWJkgYqHvfnfSAAEfGApK8C95L2o7+MiF8WCau0SQSYBNwC/AA4g5QktgHeBWwIfDYi7pH0HeBZYA3gH8BakiZExOOdCdvMrEXqrzJmRkTfwiZKWhF4D7AmMAv4jqQ9I+KikYZU2iQSESdK2hg4FZgfEc9Lmk+qNsRL56kM0qb9XUSc0ZlozcxarLlDfLcB7oqIQQBJ3wfeDiw+SUTSNGBd4DlgQtXkfuB4SXOBS4ArgbMknQSMi4hpbQ3WzKzVmptE7gXeKmkZUnPW1sDAol9SW2mTSEScXeO5a4Br8sM9qyZ/tMUhmZl1ThPPnRURN0r6LvBn0g/1m8l9xiNV2iRiZmZVmniwYUQcQxqg1BAnETOzbuDriZiZWUNKeNoTJxEzs27gSsTMzAobOu1JyTiJmJl1C1ciZmZWiK8nYmZmDXESMTOzQtyxbmZmhbk5y8zMCmviaU+aqUeTyDhgSqeDsCpzoxyXeTlb6nQIAEwryfawEnElYmZmhbhPxMzMGuJKxMzMCnHHupmZNcTNWWZmVohHZ5mZWWFuzjIzs4Y4iZiZWSEe4mtmZg1xJWJmZoW4Y93MzAoracd6CVvYzMysplF13uogabyk70q6Q9Ltkt5WJCRXImZm3aD5lcjXgF9ExK6SlgSWKbIQJxEzs27QxCQiaRzwTmAqQEQ8CzxbZFluzjIz6xb1N2dNlDRQcdu3aklrAYPA+ZJulnSOpGWLhORKxMysGwhYsu65Z0ZE3yKmjwHeBBwQETdK+hpwBPD5kYbVlEpEqv8qPpImSzqh6rlLR/qaimk1n68x375DWXlwcLDecM3MymHoYMPmdKzfD9wfETfmx98lJZURG7YSkbQHsA0wB7gdeCMwPyIOlHQrcBFwhaQPAisAN0fEuZKuBv4ArAP8BujLf68DNpP0FWACsG/FuvYD1svLOQx4F/BuYDYwX9KWwMHAtcC4iPg8MFnSeOBMYD/gm8CeuY3vRRHRD/QD9PX1+ZJxZtZ9mtQnEhEPS7pP0noR8Q9ga+DvRZZVT86aBNwCnAxsGxH7RcSBedr9EfEV4D+kPPk4sHueNj8ijiIljQeBjwI75ml3RsThwJ3AJgCSlgP2IiWMp4CNgA9ExEeAykrl+og4CRgnaSWAiJgFnEVKWtOrE4iZWddrbiUCcADwbUl/BTYGjisS1rCVSEScKGlj4NQa4c3Of3cAbo2ISyT9pmras8DsiIiKZq+o+gtpE90XEdNffEL6WL47v2K+Jar+DpkMPAIsN9x7MjPrSk0c4hsRfyG1EDWknuasacC6wHPAnySdDsyLiEMqZrsZOF7SGtT3NteTdBywEqnCISLmSPqzpK+TEsrpwA8lfREYy0sJZ9PcDzInIgYlIWkysDmwHXCRpI9ExNw64jAz6w4lPWJdEd3TPZD7RF4bEWc1spy+vr4YGBhoTlC22Dm7/nEiLTWti76bvUTSTcOMfGqJvjUVA9Prm1dTaVuMXTXENyKuAa7pcBhmZp1RwiP7uiqJmJn1rJI2ZzmJmJl1AycRMzNriJuzzMyskJGd9qRtnETMzLqBr7FuZmYNcZ+ImZkV4krEzMwa4krEzMwK8RBfMzMrTLz8tLMl4CRiwJWdDiCb0ukAgBKds2r9cpzDi7+XZHuU5nPaQa5EzMysEHesm5lZQ1yJmJlZIa5EzMysMJ/2xMzMGuJKxMzMCvFxImZmVpiTiJmZNaSEzVklDMnMzF5mqBKp51bvIqXRkm6WdEXRsFyJmJl1g9ac9uRA4HZgXNEFuBIxM+sWTaxEJL0K2BE4p5GQXImYmXWDkR1sOFHSQMXj/ojor5rnNOAwYPlGwnISMTPrFvX3d8yMiL6FTZS0E/BoRNwkactGQnISMTPrBs0d4rsZsLOkHYClgHGSLoqIPUe6IPeJmJl1i1F13oYREUdGxKsiYjKwO3B1kQQCrkTMzLpDSS9KVYpKRNJUSdu3eB37ShqQNDA4ONjKVZmZNV8LjhMBiIhrImKnomGVqRLZTdJ7gb8C6wPzgSUi4gBJl0bE7rkD6LXAbGAbYA5wOPAe4O2ksc4nRMSd1QvPIxP6Afr6+spyqTYzs/qU9LQnpahEsisjYhqwFWnUwEHATEkb1ph3EnALcHJEzAc+AcwCZgILHZFgZtbVmtQn0kxlqkRm578CXsj3o+rvMgARcaKkjYFTJR0BzI2I6e0K1MysI1RvKfJ8S8OoVKYkMuRZYFVJJwFLR8TfJP1G0rHASsDNkqYB6wLPAY8Dl0k6G5gHfCcirutU8GZmrTGKNBq3HnNbGcgCSpFEImJGxf3da0yvPtKy2gX5Zma2mBIl2WUvoHwRmZnZQpRvl12+iMzMrAZXImZmVpiTiJmZFeYkYmZmhQkY2+kgXsZJxMysK7gSMTOzwpxEzMysMCcRMzNrSPl22eWLyMzManAlYmZmhY3k3Fnt4yRiwJROB2C1/L0kl705SZ2OIDm0JNujY1yJmJlZQ8q3yy5fRGZmVoMrETMzK8xJxMzMCnPHupmZFVbOSqTNl3Q3M7PiRtd5WzRJk/Jlx2+XdJukA4tGVL60ZmZmNTS1EnkOODgi/ixpeeAmSVdFxN9HuiAnETOzrtC8JBIRDwEP5ftzJN0OrA44iZiZLZ5GlEQmShqoeNwfEf01lypNBt4I3FgkKicRM7OuIEYwOmtmRPQNu0RpOeB7wKcj4skiUTmJmJl1heaOzpK0BCmBfDsivl90OU4iZmZdoXlJRJKAc4HbI+KURpblIb5mZl1hKInUcxvWZsBewFaS/pJvOxSJypWImVnXaNrorOtIWalhTiJmZl2hnEesly8iMzOroZznzlps+kQkTZW0/SKm7ytpQNLA4OBgO0MzM2uCpvaJNE1XVSKS1gS+CAwCs4FlgYnAQcDbgWUkERG/qH5tPtCmH6Cvr6/XL5FmZl2pfLvs8kW0aPsDR0fEXbnq2AoYC2wLXA88XCuBmJl1P/eJNMsL+e/HIuJ9kqaSKpIXFv4SM7NuJ+o5Q2+7dVsSOQP4gqRHgKUlHQW8DvgVcAtwpKQxEXFFJ4M0M2u+cnasd1USiYi7gKmLmGX3NoViZtYB5dtlly8iMzOrwX0iZmZWmJOImZkV5iRiZmaFOYmYmVlhI7ooVds4iZiZdQVXImZmVpiTiJmZFeYkYmZmDSnfLrt8EZmZWQ2uRMzMrLBRpJOWl0uPJpH7SJcg6bRTOh2A2fAOLcnldz7YlEuCd7ny7bLLF5GZmdXg5iwzMyusnElksbnGupnZ4q2511iXtL2kf0j6l6QjikZVvrRmZmY1NO+iVJJGA98gXVr8fuBPkn4cEX8vEpWZmXWFplUimwL/ioj/RMSzwKXAe4pGZGZmJXfTTX++UlpiYp2zLyVpoOJxf0T0VzxenTRMdcj9wFuKxOUkYmbWBSJi+yYurtZ46UJjud2cZWbWe+4HJlU8fhXwYJEFOYmYmfWePwHrSFpT0pLA7sCPiyzIzVlmZj0mIp6T9EngSmA0cF5E3FZkWU4iZmY9KCJ+Bvys0eW4OcvMzApzEjEzs8KcRMzMrDAnETMzK8xJxMzMCitNEpE0VdJCj8iUdGn1fUlbStqvzuXvK2lA0sDg4DONB2xmZqUb4runpHcD9wBrA/OBJYDjgQ0lTQd+me8fAdwAIOnVwMGkQ/lvrTpHDAD5uX6Avr5VSnKpNjOz7laaSiS7MiIOBDYDHo2Ig4CZwATgbxExPSKuz/dPqHjd/sBc4DFgo3YHbWbWq8pWiQxVCKOAF6qeixrzDRkFXFDkXPhmZlZc2SqR7SWdDFwHrCrpJGDliPgbcIekkyStCjwv6ZiK150OfFbSVyUd2oG4zcx6UmkqkYiYAcxYxPQvVDzcs+L+NTWeMzOzNihbJWJmZl3EScTMzApzEjEzs8KcRMzMrDAnETMzK8xJxMzMCnMSMTOzwpxEzMysMCcRMzMrzEnEzMwKcxIxM7PCFNF7l9aQNEi6ZkkjJpJOU99pjmNBjmNBjmNBzYjj1RGxUjOCWRz0ZBJpBkkDEdHnOByH43AcvczNWWZmVpiTiJmZFeYkUtzLruPeIY5jQY5jQY5jQWWJY7HhPhEzMyvMlYiZmRXmJGJmZoU5iZScJHU6Bls4SUt0OoYhnf6sSBotaUlJEzoZh7WXk8gwJG0naQNJ4zqx/sidVpJ2kLS6pI7+zyRtW4adhKT/J+lgSct2YN2j8l8BZ0tatd0xVJP0IeBTkjbr0PrHAO8Fvgzs0IkYKknaVNIbOh1HL3ASWQRJOwAHAVOB0R1Y/7L51922wFER8UBEvNCpX5ySPgC8BZgrad1OxJDj2AXYBXgjsHy715//BxOA9YF7gSfbHUOlnDh2BiYB/5E0tt0xRMRzwNPAK4HHc1zLtDuOvN49gU8CB0harhMx9BInkQqVO2dJywPbAXdExKHAapI2bHNI7wROA/4DfF/Sh+Gl6qSdJG0KvA9YCZgCfEPSWm1ad3XSXCLfPg2sLGnfdsSRY3mjpF2BccC2wI7AGZLOkrReu+KoiGcUsBqwbEQcAqwC7CFpyTbHMQX4N/BDYDlJRwLT2hVH/sG1Ra6INgLuAg4DXilp/XbE0Ks8xLcGSXsD15J+dX8Q+AawG/D5iHiwA7G8FTgR2Al4KiLOb3MMe5J+Xf4JWBbYGHgzcHJEPN7GOHYGxgNPASuQfvWOB86OiP+0eN2KiMjNZ18FrouIb+eYlgGuiojHWhlDjc9lg5sAAAkrSURBVJg+AfwTuAa4jPT/WQs4NiLubmMcU4BDgLuB64EngGdIP8AaPUddvTFsDGwJ3AdskuOYD7wLOL3d39teMqbTAZSNpP8hNZNcFBF3SZoFvB6Y3s4PoqTtgAnA5cBs4FjgKGBWu2LIcSxDqkDOjIhBSXOAPqC/zQnk46Tmo18AnwKOAR4GbouIe1u87lG5CWtFYB3gs8Bxkp4HbgK2GkogQ8mmlfHk9byJ9OPiVlJleBCpCjknIh5t9for4tgNWJv0o+sHwO6kk5uen5u4Wr3+FYBNgbnAALAm8CywMvBq0vZwAmkhJ5GXWxK4ISKel7QFqUT/VTu+EEPyL7uPk37RvQ/YFwhgXkS0LYnkZqK/Az8F3idpbkT8XtIxEfF8i9c9KiJeqHhqFnBaRPxb0iRSFf3zVsYwJCeQVUg7yFVIO8kvAScBD0fE2RXztiOBvB14E3A76Yy0RwPfi4gzW73uqjj2AV5H6kw/mNQnMwb4eRu/L8+R3v/DEfF+Sc8CY4F7IuK8NsXQ03q+T2SovV3SzpLeD/yR1KZ8ALAP8EyrvxBVfTFjSL+w7wQOBG4EvhwRP4qIh1sZR1VMuwNrkNrbZwFXkrbL0sALi3ptkwyNSpuWk/nrgA9J2gvYgtRP1HKS+iStDHyIdArwzwJLA5sDe0XENe2Io8pKpOai35Kq1BuAKzoQx2PAhRHxBKm583+B/4uI+1u9Ykkr5L6OiaTKcI6kbUgVye2kqsTaoKf7RCSNBz4CPAK8G1gP+CZwNbAc8EBEDLYhjqH29mnAHFKn4CuBFUk78nPa8cWsiukQ4MGIuFjSdFKb+51tqEAECDiVlEjXBH5PGoU1npRUH251W3tFE9bHSTvt60m/tOflx7Mi4sKhmNtUgewF/Csi/pBH7L2Z1A/x64h4pNXrrxHPeqSK+S+kyugrEfFAm9a9AXAJMBARH8mjsM4gVSaHt+N7a0mvVyJrkjrM1yXtnHYE3pafu6XVH0Qlo4ATJH2SlDDuJY362YTUbHJeOxOIpI/l5pIZwJskfY6UzOa2OoFAag7KzVhPktr6L4yIH5A6058GbmpHZ21OIBNJ7f0iVR73kjqu3ziUQIZibnU8uT/mVtKxIGsDz5N+hf+8EwkEICL+AXwlx3VSuxJI9gCpUl9a0pqk/9GROIG0Xa9XIhOBLwC3kJpLHiM1E/yg1aN9quI4hjTi6bSI+K2ktwDbAKdGxNNtjGMrUrv/10hV2c9Jxx/8rs2DCvYANiA1Lb4fmE6qzpaJiKdavO6hqlDAR4HHI+J7uf1/HeBnwGRSZfZsmxLIVGBr4GJSU9o2pJ3ohRFxX6vXXzZ5e2wFXEoa6v3/SBXiEblpzdqopzvWI2JmHs/+ZtKvupVIv6jaucP8KOmL8FVgz9x5faOkWyJiXovX/WIzjKTXkJrwnidVqIeSjj34VitjqBHTUqQEMoe0XTYjddzu1cYEshxpgMVo0jEH3yMNKb4tIq6VdGNEzG9lLBUx7UDaHp8A9iAN9PgUsHREzGlHDGUiaSfS9vgkaXv8C9iPtD2cQDqgp5MIQEQ8KWmAVA7/KSJmtzmEx4A/RsRfJb0ROEjSh1udQGCBU6ocAIyJiFMl3UnqOL8639oqIuZJOpvUtv1m4EfAKRHx31autyKBjCMNp16e1C+zS27Su38oobY6gSidj0uk04f0kYarLkdqTtsFuLaXEkgebDKK1Ly5OWl7LENq7n0vqVJu23BzW1BPN2dVqjGktF3rXQ/YH7gZeANph9nyJoo8ymoP0k76GFKfw22kDv1/ADe3uY27Or4lgQOAS9sRh6TVIuJBSR8kNeFdHhHfl7Q66Xtyf56vpZ3ouY/sA6Rjk8YCF5CGEz9H+v98s5easPL22I107NYo4FukA29nAg8BX2v3oBNbkJNICSidwG814NF27iDyyKMNgItII6E+SjoO4rPtaq5ZFElLtKECGUU6qPNc4ARSh/65wJ6kMwTMjnyGgDaOwnolKZHMiogLJG1Car45qhcPnMvbYw/S2Rq+mQ9wXJHUd9m2AyutNieRHiVpp4i4QtKBwOqkPpn3Az+NNp4yo5Mqq09JryY1Y51I2kHtSjptxxkdim0V4D2k4y++T2pufLYTsZRB3h7vJR2FviJwTKdGpdmCnER6UD6ocvOIODA/PpR0DMRRPdbWvhppJ30iadTV8qQd1ddJxx/8N8/XlgpkIfFtB3y31YMKukGuSHYFfugmrPJwEukxSucGexPwo4j4i9I5mG4Glo+Ijp7SvJ2UztL8OWBD4EzSiSWfIXVevy3yKUQ6lUAq4hwTbTzlTtlJGt2O45Wsfr1+sGFPyacyeSfpbK/HSHof6YCtlXslgVScYuZp4CzSWXBHRcRlpPOUPRYV56DqZALJ63cCqeAEUj5OIj0iH3+xIWlUy1jS0fETgSN7pW25YhjvROAzpCG0pwB9kn5MGjrbMyOfzJrBzVk9ROnst0PHX/wPaRRWT5wiouo4kF2BQVI/0GRSRfLOiJhROW+nYjXrJq5Eekj+lf0Y6fQd03swgaxIGsa7AXBXHnn1FPBmJxCzYlyJ9KB2HH9RNvlUJucC/aRTu2xAar66taOBmXU5VyI9qFcSSD6QcGhEz1OkS6e+LdI1QB4mHVg5NG/1ddzNrA6uRGyxpnT51H1IF7k6Dfg0sGpEHNbRwMwWE04ittip6AMZS7ps6+uB+aSTGn4YmBJturSu2eLOzVm2WKlIICuQLjZ2A+ksyVNJQ5v3GkogbsIya5yTiC02KhLIssBPgAkRcTUwX1I/6bxgFwzN71FYZo1zc5YtFiorkIiYLek4YImIODRXJRtGxO8r5+1sxGaLB1ci1vUqEsh44BuSNo+IzwKPSDonImY7gZi1hisR62pDp3PPR6JPAVaIiHMqpm+Zh/SaWQu4ErGulhPIeOA4YGlgR0lrSvqipF2HEog70c1aw0nEulpODjsCSwF/IF2l8BPAfyLiu0PzuQnLrDXGdDoAs0bkvpDf8dIPoidJF5S6FNwHYtZq7hOxxUa+YuO4iDg3P3YCMWsxJxFbbEiaEBGP5/tOIGZt4CRiix0nELP2cRIxM7PCPDrLzMwKcxIxM7PCnETMzKwwJxEzMyvMScTMzAr7//UkuVXqqdB4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the confusion matrix\n",
    "plot_matrix(cm, cmap='hot_r', labels=unique_conditions)\n",
    "plt.title('Artificial neural networks confusion matrix')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
